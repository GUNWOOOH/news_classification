{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyMZ4nFAsoAqIwTjizCaOey1",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/GUNWOOOH/news_classification/blob/main/bert_news_classification.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "코드 출처: https://zzaebok.github.io/deep_learning/nlp/Bert-for-classification/"
      ],
      "metadata": {
        "id": "QtTfrCVHmD9K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/qdrive')\n",
        "!pip install konlpy\n",
        "!pip install pytorch-transformers\n",
        "\n",
        "#텍스트 분류\n",
        "\n",
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from pytorch_transformers import BertTokenizer, BertForSequenceClassification, BertConfig\n",
        "from torch.optim import Adam\n",
        "import torch.nn.functional as F\n",
        "\n",
        "path = \"/content/qdrive/MyDrive/논문데이터\"\n",
        "file_list = os.listdir(path)\n",
        "file_list2 = []\n",
        "\n",
        "for i in file_list:\n",
        "    file_list2.append(int(str(i).split('.')[0]))\n",
        "\n",
        "df = pd.DataFrame({'날짜' : [],'신문사' : [],'제목' : [],'내용' : [],'분야' : []})\n",
        "\n",
        "for file in file_list2:\n",
        "    df_sam = pd.read_csv(path+ '/'+ str(file) +\".csv\", encoding='UTF8')\n",
        "    df = pd.concat([df,df_sam])\n",
        "\n",
        "\n",
        "import re\n",
        "tit = []\n",
        "\n",
        "df['신문사']  = np.where(df['신문사'] == 'TV조선', '조선일보', df['신문사'])\n",
        "df['신문사']  = np.where(df['신문사'] == '한겨레21', '한겨레', df['신문사'])\n",
        "df = df[(df['분야'] != '세계') & (df['분야'] != 'IT') & (df['분야'] != '오피니언')]\n",
        "df = df[(df['신문사'] != '중앙일보') & (df['신문사'] != '경향신문')]\n",
        "\n",
        "for i in df.제목:\n",
        "    string = re.sub(r'[^ A-Za-z가-힣+]', ' ', i)\n",
        "    tit.append(' '.join(string.split()))\n",
        "\n",
        "#특정신문사 일 때 1로 만들어주기\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1RQWCH804qCW",
        "outputId": "1b985abf-306a-42d4-97f8-aceb2d0c6a4f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/qdrive; to attempt to forcibly remount, call drive.mount(\"/content/qdrive\", force_remount=True).\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: konlpy in /usr/local/lib/python3.10/dist-packages (0.6.0)\n",
            "Requirement already satisfied: JPype1>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from konlpy) (1.4.1)\n",
            "Requirement already satisfied: lxml>=4.1.0 in /usr/local/lib/python3.10/dist-packages (from konlpy) (4.9.2)\n",
            "Requirement already satisfied: numpy>=1.6 in /usr/local/lib/python3.10/dist-packages (from konlpy) (1.22.4)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from JPype1>=0.7.0->konlpy) (23.1)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: pytorch-transformers in /usr/local/lib/python3.10/dist-packages (1.2.0)\n",
            "Requirement already satisfied: torch>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from pytorch-transformers) (2.0.1+cu118)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from pytorch-transformers) (1.22.4)\n",
            "Requirement already satisfied: boto3 in /usr/local/lib/python3.10/dist-packages (from pytorch-transformers) (1.26.138)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from pytorch-transformers) (2.27.1)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from pytorch-transformers) (4.65.0)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.10/dist-packages (from pytorch-transformers) (2022.10.31)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.10/dist-packages (from pytorch-transformers) (0.1.99)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.10/dist-packages (from pytorch-transformers) (0.0.53)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.0.0->pytorch-transformers) (3.12.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch>=1.0.0->pytorch-transformers) (4.5.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.0.0->pytorch-transformers) (1.11.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.0.0->pytorch-transformers) (3.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.0.0->pytorch-transformers) (3.1.2)\n",
            "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.0.0->pytorch-transformers) (2.0.0)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch>=1.0.0->pytorch-transformers) (3.25.2)\n",
            "Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch>=1.0.0->pytorch-transformers) (16.0.5)\n",
            "Requirement already satisfied: botocore<1.30.0,>=1.29.138 in /usr/local/lib/python3.10/dist-packages (from boto3->pytorch-transformers) (1.29.138)\n",
            "Requirement already satisfied: jmespath<2.0.0,>=0.7.1 in /usr/local/lib/python3.10/dist-packages (from boto3->pytorch-transformers) (1.0.1)\n",
            "Requirement already satisfied: s3transfer<0.7.0,>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from boto3->pytorch-transformers) (0.6.1)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->pytorch-transformers) (1.26.15)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->pytorch-transformers) (2022.12.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests->pytorch-transformers) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->pytorch-transformers) (3.4)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from sacremoses->pytorch-transformers) (1.16.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from sacremoses->pytorch-transformers) (8.1.3)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from sacremoses->pytorch-transformers) (1.2.0)\n",
            "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.10/dist-packages (from botocore<1.30.0,>=1.29.138->boto3->pytorch-transformers) (2.8.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.0.0->pytorch-transformers) (2.1.2)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.0.0->pytorch-transformers) (1.3.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#분야 고르기\n",
        "col_df = pd.DataFrame({'신문사': df.신문사,'분야': df.분야,'제목2' : tit})\n",
        "col_df_sam = col_df.head(20)\n",
        "#한국어 전처리 KONLPY\n",
        "from konlpy.tag import Okt\n",
        "from sklearn.model_selection import train_test_split\n",
        "pos_tagger = Okt()\n",
        "docs = [pos_tagger.pos(row) for row in col_df['제목2']] # Training Data ( 학습용 데이터 )\n",
        "#docs = [pos_tagger.pos(row) for row in col_df_sam['제목2']] # Training Data ( 학습용 데이터 )\n",
        "contents_tokens = []\n",
        "\n",
        "for content in docs:\n",
        "  okt_filtering = [x for x, y in content if y in ['Noun', 'Adjective', 'Verb']]\n",
        "  contents_tokens.append(okt_filtering)\n",
        "\n",
        "contents_for_vectorize = []\n",
        "for content in contents_tokens:\n",
        "  sentence = ''\n",
        "  for word in content:\n",
        "    sentence = sentence + ' ' + word\n",
        "  contents_for_vectorize.append(sentence.lstrip())\n"
      ],
      "metadata": {
        "id": "X6AbeFV38nFM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "col_df['제목2'] = contents_for_vectorize\n",
        "col_df['Y'] = list((col_df['신문사'] == '한겨레')*1)\n",
        "col_df2 = col_df[col_df['분야']=='정치']\n",
        "col_df2['Y'] = col_df2['Y'].astype('category')\n",
        "col_df2.shape\n",
        "\n",
        "# col_df_sam['제목2'] = contents_for_vectorize\n",
        "# col_df_sam['Y'] = list((col_df_sam['신문사'] == '중앙일보')*1)\n",
        "# col_df_sam2 = col_df_sam[col_df_sam['분야']=='경제']\n",
        "# col_df_sam2['Y'] = col_df_sam2['Y'].astype('category')\n",
        "# col_df_sam2.shape"
      ],
      "metadata": {
        "id": "E6eVkNVFJvB2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f8271681-e101-4e3b-e4a3-125569bb5210"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-3-335ce547898d>:4: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  col_df2['Y'] = col_df2['Y'].astype('category')\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(115255, 4)"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 데이터 전처리"
      ],
      "metadata": {
        "id": "rFe1qMR04VEC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "#실험코드\n",
        "col_df2 = col_df[col_df['분야']=='정치']\n",
        "col_df2['Y'] = col_df2['Y'].astype('category')\n",
        "col_df2.shape\n",
        "\n",
        "train_df,test_df = train_test_split(\n",
        "  col_df2 , random_state=104,test_size=0.25, shuffle=True)\n",
        "  # col_df_sam2 , random_state=104,test_size=0.25, shuffle=True)"
      ],
      "metadata": {
        "id": "v7LK3FsO-HdO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "48dad763-1df0-4745-d8c3-32108e5d3cd9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-4-1c2570f512ae>:5: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  col_df2['Y'] = col_df2['Y'].astype('category')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class NsmcDataset(Dataset):\n",
        "    ''' Naver Sentiment Movie Corpus Dataset '''\n",
        "    def __init__(self, df):\n",
        "        self.df = df\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.df)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        text = self.df.iloc[idx, 2]\n",
        "        label = self.df.iloc[idx, 3]\n",
        "        return text, label"
      ],
      "metadata": {
        "id": "CqU83fLO9Gm_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nsmc_train_dataset = NsmcDataset(train_df)\n",
        "train_loader = DataLoader(nsmc_train_dataset, batch_size=2, shuffle=True, num_workers=2)"
      ],
      "metadata": {
        "id": "pn8XFVqM9Nmh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda\")\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-multilingual-cased')\n",
        "model = BertForSequenceClassification.from_pretrained('bert-base-multilingual-cased')\n",
        "model.to(device)"
      ],
      "metadata": {
        "id": "W4zqKdXw9P3N",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3149da44-3521-40ff-bec5-4c893cec6d6c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "BertForSequenceClassification(\n",
              "  (bert): BertModel(\n",
              "    (embeddings): BertEmbeddings(\n",
              "      (word_embeddings): Embedding(119547, 768, padding_idx=0)\n",
              "      (position_embeddings): Embedding(512, 768)\n",
              "      (token_type_embeddings): Embedding(2, 768)\n",
              "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "      (dropout): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (encoder): BertEncoder(\n",
              "      (layer): ModuleList(\n",
              "        (0-11): 12 x BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (pooler): BertPooler(\n",
              "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "      (activation): Tanh()\n",
              "    )\n",
              "  )\n",
              "  (dropout): Dropout(p=0.1, inplace=False)\n",
              "  (classifier): Linear(in_features=768, out_features=2, bias=True)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "optimizer = Adam(model.parameters(), lr=1e-6)\n",
        "\n",
        "itr = 5\n",
        "p_itr = 200\n",
        "epochs = 5\n",
        "total_loss = 0\n",
        "total_len = 0\n",
        "total_correct = 0\n",
        "\n",
        "\n",
        "model.train()\n",
        "pred_all = []\n",
        "for epoch in range(epochs):\n",
        "    \n",
        "    for text, label in train_loader:\n",
        "        optimizer.zero_grad()\n",
        "        \n",
        "        # encoding and zero padding\n",
        "        encoded_list = [tokenizer.encode(t, add_special_tokens=True) for t in text]\n",
        "        padded_list =  [e + [0] * (512-len(e)) for e in encoded_list]\n",
        "        \n",
        "        sample = torch.tensor(padded_list)\n",
        "        sample, label = sample.to(device), label.to(device)\n",
        "        labels = torch.tensor(label)\n",
        "        outputs = model(sample, labels=labels)\n",
        "        loss, logits = outputs\n",
        "\n",
        "        pred = torch.argmax(F.softmax(logits), dim=1)\n",
        "        pred_all.append(pred)\n",
        "\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        \n",
        "# evaluation\n",
        "model.eval()\n",
        "\n",
        "nsmc_eval_dataset = NsmcDataset(test_df)\n",
        "eval_loader = DataLoader(nsmc_eval_dataset, batch_size=2, shuffle=False, num_workers=2)\n",
        "total_loss = 0\n",
        "total_len = 0\n",
        "total_correct = 0\n",
        "y_pred = []\n",
        "y_true = []\n",
        "\n",
        "for text, label in eval_loader:\n",
        "    encoded_list = [tokenizer.encode(t, add_special_tokens=True) for t in text]\n",
        "    padded_list =  [e + [0] * (512-len(e)) for e in encoded_list]\n",
        "    sample = torch.tensor(padded_list)\n",
        "    sample, label = sample.to(device), label.to(device)\n",
        "    labels = torch.tensor(label)\n",
        "    outputs = model(sample, labels=labels)\n",
        "    _, logits = outputs\n",
        "\n",
        "    pred = torch.argmax(F.softmax(logits), dim=1)\n",
        "    correct = pred.eq(labels)\n",
        "    total_correct += correct.sum().item()\n",
        "    total_len += len(labels)\n",
        "    y_pred.extend((torch.max(torch.exp(logits), 1)[1]).data.cpu().numpy())\n",
        "    y_true.extend(label.data.cpu().numpy())\n",
        "\n",
        "print('Test accuracy: ', total_correct / total_len)\n",
        "\n",
        "from sklearn.metrics import classification_report\n",
        " \n",
        "print(classification_report(y_true, y_pred))\n",
        "     "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lvoZRP4U9Q1P",
        "outputId": "2388b224-27f8-4986-dffd-931cbff55078"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-8-d9a28cd834c2>:24: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  labels = torch.tensor(label)\n",
            "<ipython-input-8-d9a28cd834c2>:28: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  pred = torch.argmax(F.softmax(logits), dim=1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# evaluation\n",
        "model.eval()\n",
        "\n",
        "nsmc_eval_dataset = NsmcDataset(test_df)\n",
        "eval_loader = DataLoader(nsmc_eval_dataset, batch_size=2, shuffle=False, num_workers=2)\n",
        "total_loss = 0\n",
        "total_len = 0\n",
        "total_correct = 0\n",
        "y_pred = []\n",
        "y_true = []\n",
        "\n",
        "for text, label in eval_loader:\n",
        "    encoded_list = [tokenizer.encode(t, add_special_tokens=True) for t in text]\n",
        "    padded_list =  [e + [0] * (512-len(e)) for e in encoded_list]\n",
        "    sample = torch.tensor(padded_list)\n",
        "    sample, label = sample.to(device), label.to(device)\n",
        "    labels = torch.tensor(label)\n",
        "    outputs = model(sample, labels=labels)\n",
        "    _, logits = outputs\n",
        "\n",
        "    pred = torch.argmax(F.softmax(logits), dim=1)\n",
        "    correct = pred.eq(labels)\n",
        "    total_correct += correct.sum().item()\n",
        "    total_len += len(labels)\n",
        "    y_pred.extend((torch.max(torch.exp(logits), 1)[1]).data.cpu().numpy())\n",
        "    y_true.extend(label.data.cpu().numpy())\n",
        "\n",
        "print('Test accuracy: ', total_correct / total_len)\n",
        "\n",
        "from sklearn.metrics import classification_report\n",
        "print(classification_report(y_true, y_pred))"
      ],
      "metadata": {
        "id": "pqrQA0PdSGDI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7f2d03db-e616-4727-abe7-57cf3cbc65ab"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-121-8fee813334e4>:17: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  labels = torch.tensor(label)\n",
            "<ipython-input-121-8fee813334e4>:21: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  pred = torch.argmax(F.softmax(logits), dim=1)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test accuracy:  0.8275503122831367\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import confusion_matrix\n",
        "cf_matrix = confusion_matrix(y_true, y_pred)"
      ],
      "metadata": {
        "id": "sn9l6VmUPo-E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import classification_report\n",
        " \n",
        "print(classification_report(y_true, y_pred))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DTBX8ckZJWYG",
        "outputId": "3ea9628e-c4c0-4dfa-a6d7-4e7943950e10"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.83      1.00      0.91      2385\n",
            "           1       0.00      0.00      0.00       497\n",
            "\n",
            "    accuracy                           0.83      2882\n",
            "   macro avg       0.41      0.50      0.45      2882\n",
            "weighted avg       0.68      0.83      0.75      2882\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "(torch.max(torch.exp(logits), 1)[1])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F5jPkVxkQTkN",
        "outputId": "2269e98a-f31c-4244-a23a-8aaac20e5eba"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([0], device='cuda:0')"
            ]
          },
          "metadata": {},
          "execution_count": 114
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.max(torch.exp(logits), 1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NuZCUF7nmQV2",
        "outputId": "6b16de6a-fe3a-4828-9122-782a745ff47d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.return_types.max(\n",
              "values=tensor([2.1898], device='cuda:0', grad_fn=<MaxBackward0>),\n",
              "indices=tensor([0], device='cuda:0'))"
            ]
          },
          "metadata": {},
          "execution_count": 116
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.exp(logits)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4MAcYoOZmUkG",
        "outputId": "a5908bb8-6a47-43ef-f307-d49426c1b615"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[2.1898, 0.3519]], device='cuda:0', grad_fn=<ExpBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 117
        }
      ]
    }
  ]
}